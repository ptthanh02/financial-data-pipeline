[2024-05-26T10:51:11.727+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: crawl_stock_data.create_table_and_insert_task manual__2024-05-26T10:50:59.714909+00:00 [queued]>
[2024-05-26T10:51:11.738+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: crawl_stock_data.create_table_and_insert_task manual__2024-05-26T10:50:59.714909+00:00 [queued]>
[2024-05-26T10:51:11.739+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 2
[2024-05-26T10:51:11.754+0000] {taskinstance.py:2214} INFO - Executing <Task(PythonOperator): create_table_and_insert_task> on 2024-05-26 10:50:59.714909+00:00
[2024-05-26T10:51:11.761+0000] {standard_task_runner.py:60} INFO - Started process 771 to run task
[2024-05-26T10:51:11.764+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'crawl_stock_data', 'create_table_and_insert_task', 'manual__2024-05-26T10:50:59.714909+00:00', '--job-id', '6', '--raw', '--subdir', 'DAGS_FOLDER/price_stock.py', '--cfg-path', '/tmp/tmp55czfq2z']
[2024-05-26T10:51:11.765+0000] {standard_task_runner.py:88} INFO - Job 6: Subtask create_table_and_insert_task
[2024-05-26T10:51:11.822+0000] {task_command.py:423} INFO - Running <TaskInstance: crawl_stock_data.create_table_and_insert_task manual__2024-05-26T10:50:59.714909+00:00 [running]> on host cb27ea2525bc
[2024-05-26T10:51:11.912+0000] {taskinstance.py:2510} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='crawl_stock_data' AIRFLOW_CTX_TASK_ID='create_table_and_insert_task' AIRFLOW_CTX_EXECUTION_DATE='2024-05-26T10:50:59.714909+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-05-26T10:50:59.714909+00:00'
[2024-05-26T10:51:12.019+0000] {base.py:83} INFO - Using connection ID 'postgres_localhost' for task execution.
[2024-05-26T10:51:12.037+0000] {sql.py:457} INFO - Running statement: 
            CREATE TABLE IF NOT EXISTS stock_price (
                code VARCHAR(50),
                date DATE,
                time TIME,
                floor VARCHAR(50),
                type VARCHAR(50),
                basicPrice FLOAT,
                ceilingPrice FLOAT,
                floorPrice FLOAT,
                open FLOAT,
                high FLOAT,
                low FLOAT,
                close FLOAT,
                average FLOAT,
                adOpen FLOAT,
                adHigh FLOAT,
                adLow FLOAT,
                adClose FLOAT,
                adAverage FLOAT,
                nmVolume FLOAT,
                nmValue FLOAT,
                ptVolume FLOAT,
                ptValue FLOAT,
                change FLOAT,
                adChange FLOAT,
                pctChange FLOAT
            );
        , parameters: None
[2024-05-26T10:51:12.054+0000] {logging_mixin.py:188} INFO - Bảng stock_price đã được tạo
[2024-05-26T10:51:12.064+0000] {base.py:83} INFO - Using connection ID 'postgres_localhost' for task execution.
[2024-05-26T10:51:12.080+0000] {sql.py:457} INFO - Running statement: SELECT MAX(date) FROM stock_price, parameters: None
[2024-05-26T10:51:12.086+0000] {sql.py:466} INFO - Rows affected: 1
[2024-05-26T10:51:12.088+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.8/site-packages/***/providers/common/sql/hooks/sql.py:407: AirflowProviderDeprecationWarning: Call to deprecated method _make_common_data_structure. (The `_make_serializable` method is deprecated and support will be removed in a future version of the common.sql provider. Please update the DbApiHook's provider to a version based on common.sql >= 1.9.1.)
  result = self._make_common_data_structure(handler(cur))

[2024-05-26T10:51:12.369+0000] {base.py:83} INFO - Using connection ID 'postgres_localhost' for task execution.
[2024-05-26T10:51:15.673+0000] {sql.py:568} INFO - Loaded 1000 rows into stock_price so far
[2024-05-26T10:51:19.516+0000] {sql.py:568} INFO - Loaded 2000 rows into stock_price so far
[2024-05-26T10:51:23.375+0000] {sql.py:568} INFO - Loaded 3000 rows into stock_price so far
[2024-05-26T10:51:26.220+0000] {sql.py:568} INFO - Loaded 4000 rows into stock_price so far
[2024-05-26T10:51:29.208+0000] {sql.py:568} INFO - Loaded 5000 rows into stock_price so far
[2024-05-26T10:51:32.602+0000] {sql.py:568} INFO - Loaded 6000 rows into stock_price so far
[2024-05-26T10:51:35.419+0000] {sql.py:568} INFO - Loaded 7000 rows into stock_price so far
[2024-05-26T10:51:38.592+0000] {sql.py:568} INFO - Loaded 8000 rows into stock_price so far
[2024-05-26T10:51:42.132+0000] {sql.py:568} INFO - Loaded 9000 rows into stock_price so far
[2024-05-26T10:51:46.315+0000] {sql.py:572} INFO - Done loading. Loaded a total of 9990 rows into stock_price
[2024-05-26T10:51:46.316+0000] {logging_mixin.py:188} INFO - Dữ liệu đã được chèn thành công vào bảng.
[2024-05-26T10:51:46.325+0000] {python.py:202} INFO - Done. Returned value was: None
[2024-05-26T10:51:46.344+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=crawl_stock_data, task_id=create_table_and_insert_task, execution_date=20240526T105059, start_date=20240526T105111, end_date=20240526T105146
[2024-05-26T10:51:46.416+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-05-26T10:51:46.456+0000] {taskinstance.py:3309} INFO - 0 downstream tasks scheduled from follow-on schedule check
